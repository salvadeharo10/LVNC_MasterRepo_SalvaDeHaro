seed: 1122
data_folder: ../../LVNC_dataset
preproc_name: New_Preproc_2d_800_combinedresize_zscore

data_loader:
  train:
    num_workers: 4
    batch_size: 8
    shuffle: true
    pin_memory: true
    prefetch_factor: 2
  val:
    num_workers: 4
    batch_size: 8
    shuffle: false
    pin_memory: true
    prefetch_factor: 2

unet:
  init: kaiming
  classes:
  - bg
  - el
  - ic
  - t
  channels:
  - 1
  - 32
  - 64
  - 128
  - 256
  - 512
  - 1024
  pool: true
  conv_config:
    __normalization: torch.nn.InstanceNorm2d
    normalization_params:
      eps: 1.0e-05
      momentum: 0.1
      affine: true
      track_running_stats: false
    __activation: torch.nn.LeakyReLU
    activation_params:
      inplace: true
      negative_slope: 0.01

transformer:
  hidden_size: 768
  mlp_dim: 3072
  num_heads: 12
  num_layers: 12
  attention_dropout_rate: 0.0
  dropout_rate: 0.1
  patches:
    size:
    - 32
    - 32
    grid:
    - 32
    - 32
  classifier: 'seg'
  representation_size: None
  pretrained_path: './model_preTrained/tu_checkpoint/imagenet21k/imagenet21k_ViT-B_32.npz'
  patch_size: 32
  img_size: 800
  n_classes: 4
  activation: 'softmax'

trainer:
  accelerator: "gpu"
  precision: 32
  deterministic: false
  max_epochs: 100
  min_epochs: 1
  gradient_clip_val: 1.0

callbacks:
  early_stopping:
    apply: true
    params:
      monitor: val/loss
      patience: 11
      mode: min
  test_segmentation_results:
    apply: true
    params:
      palette:
      - - 0
        - 0
        - 0
      - - 17
        - 165
        - 121
      - - 67
        - 191
        - 222
      - - 242
        - 183
        - 1
      group_results_by: set

loggers:
  use_tensorboard: true
  use_csv_logger: true

loss:
  __function: loss.composite.CombinedLoss
  channel_dim: false
  params:
    lovasz_weight: 0.6          # damos un poco mas de peso al Lovasz porque es bueno para maximizar IoU
    focal_weight: 0.4           # menos peso al Dice+Focal para evitar sobreajuste a las clases pequenas
    normalize_weights: true     # normaliza los pesos para que sumen 1 (buena practica)
    log_partial_losses: true    # registra por separado las perdidas para analisis posterior en los logs
    lovasz_params:
      classes: present          # ignora clases ausentes en cada batch
      apply_softmax: true       # aplica softmax dentro de Lovasz (recomendado si pasas logits)
    focal_params:
      gamma: 2.0                # enfasis en errores dificiles (gamma tipico de focal loss)
      alpha: 0.25               # balancea clases desbalanceadas (0.25 es valor comun)
      smooth: 1.0               # suavizado para evitar division por cero en dice



optimizer:
  __function: torch_optimizer.Lookahead
  params:
    optimizer: torch_optimizer.RAdam
    lr: 0.0025
    weight_decay: 1.0e-05
    k: 5
    alpha: 0.5